---
featured: true
title: 透過 Langchain 及開源 llama AI 在Next JS 打造自己的AI Bot API - part's 1
date:  2024-10-01 01:01:35 +0800
thumbnail:  langchain.jpg
category:  AI  
tags:   ['llama','ai','langchain' ]
description : 透過 Langchain 及 開源 llama AI 在Next JS 打造自己的AI Bot API - part's 1
author : Mark Ku
slug: ai-bot-api-parts-1
---
## 前言 
先前我利用[Langchain 來優化德國某電子商務網站的 SEO](https://blog.markkulab.net/generate-seo-metadata-by-azure-ai/)，當時使用的是 Azure AI，不過，由於月底總是預算耗盡，我開始尋找替代方案，最終找到Facebook 開源的AI 模型 llama 替代 。

其實我有用了 Python 和 Langchain 寫了一個 AI bot 的算命機器人，但考慮到團隊成員主要精通 JavaScript，而不是 Python，為了統一的技術棧，大家比較能相互備援，我決定嘗試使用 Next 的API 開發這個 AI 相關功能，最後做成 AI Agent BOT API。

![Line AI BOT](line-ai-bot.png)

## 複習一下 - Langchain 是什麼
Langchain 是一個可以整合多種大型 AI 模型的框架，透過它可以輕鬆調用各類大型語言模型。這個框架提供了模版、解析器、動態路由，並且可以與 FastAPI 、Next js 或等其他系統整合。

## 為什麼需要使用 Langchain
如果使用情境很單純，只需要與大型語言模型單純一問一答，那麼，我覺得可以不需要使用到langchain ，但如果你希望依據使用者輸入的語意讓 AI 決定要做什麼事，或是需要在一次使用請求中和大型模型語言交互多次，像是從自己的向量資料庫查不到相關資料，再與外部api 交換，那麼 langchain 就很適合你。

![Why use langchain](why-use-langchain.png)

## 預先準備
* 安裝 Ollama [參考先前寫的文章](https://blog.markkulab.net/build-your-ollama-ai-with-hardware-acceleration/)
* 預先準備 Next JS 專案
* 安裝langchain 相關套件

```
npm install langchain @langchain/azure-openai @langchain/ollama @langchain/openai --save
```
## 範例一，透過 AI 產生店名
## 原本透過 Azure Open AI 來生成鞋店的店名

```
import { AzureChatOpenAI } from '@langchain/openai';
import { NextApiRequest, NextApiResponse } from 'next';

export default async function handler(req: NextApiRequest, res: NextApiResponse) {
    const model = new AzureChatOpenAI({
        azureOpenAIApiKey: process.env.AZURE_OPENAI_API_KEY,
        azureOpenAIApiDeploymentName: 'gpt-4',
        azureOpenAIApiInstanceName: 'your-instance-name',
        azureOpenAIApiVersion: '2023-03-15-preview',
        temperature: 0,
        maxTokens: 500,
    });

    try {
        const result = await model.invoke('為一個鞋店起一個好的名字。');
        res.status(200).json({ result });
    } catch (error) {
        res.status(500).json({ error: 'Failed to generate name' });
    }
}


```
現在透過 ChatOllama 來生成鞋店的店名
Langchain 是不是很方便，它把界面設計的很好，直接把 AzureChatOpenAI 換成 ChatOllama，後程式也都不用改，就把模型換掉
```
import { ChatOllama } from '@langchain/ollama';
import { NextApiRequest, NextApiResponse } from 'next';

export default async function handler(req: NextApiRequest, res: NextApiResponse) {
    const model = new ChatOllama({
        model: 'llama3.2',
        temperature: 0,
        maxRetries: 2,
        baseUrl: 'http://localhost:11434',
    });

    try {
        const result = await model.invoke('為一個鞋店起一個好的名字。');
        const content = JSON.stringify(result.content);
        res.status(200).json(content);
    } catch (error) {
        res.status(500).json({ error: 'Failed to generate name' });
    }
}

```
## 範例二，使用chatTemplate 

```
import { ChatOllama } from '@langchain/ollama';
import { NextApiRequest, NextApiResponse } from 'next';

export default async function handler(req: NextApiRequest, res: NextApiResponse) {
    const { brandName, modelVersion = 'llama3.2', temperature = 0 } = req.body;

    const model = new ChatOllama({
        model: modelVersion,
        temperature,
        maxRetries: 2,
        baseUrl: 'http://localhost:11434',
    });

    try {
        const result = await model.invoke(`為一個鞋店起一個好的名字：${brandName}`);
        const content = JSON.stringify(result.content);
        res.status(200).json(content);
    } catch (error) {
        res.status(500).json({ error: 'Failed to generate name' });
    }
}
```
## 範例三，使用 chain - 讓 AI 去幫我呼叫 api 

```
import { ChatOllama } from '@langchain/ollama';
import { APIChain } from 'langchain/chains';
import { NextApiRequest, NextApiResponse } from 'next/types';

const OPEN_METEO_DOCS = `api document ...`;

// Next.js API route
export default async function handler(req: NextApiRequest, res: NextApiResponse) {
    const { question, modelVersion = 'llama3.2', temperature = 0 } = req.body;

    try {
        const model = new ChatOllama({
            model: modelVersion,
            temperature,
            maxRetries: 2,
            baseUrl: 'http://localhost:11434',
        });

        const chain = APIChain.fromLLMAndAPIDocs(model, OPEN_METEO_DOCS, {
            headers: {
                // API-specific headers if required
            },
        });

        const weatherResponse = await chain.invoke({ question });

        res.status(200).json({ weather: weatherResponse });
    } catch (error) {
        console.error('Error fetching weather:', error);
        res.status(500).json({ message: 'Internal Server Error' });
    }
}

```